# PROJECT - Wrangle Twitter data via API

## Table of contents 
* [Introduction](#introduction)
* [Setup](#setup)
* [References](#references)

## Introduction
Data in the real world and that provided for academia are completely different.
Data provided for academia is for teaching purposes and is generally ideal with minimal corrections required to provide examples.
Real world data is messy and untidy with no real structure, and would otherwise overwhelm students.

This projects intent is to simulate in between where real world data has been structured to suit the providers needs, gathering to put to practice that which has been learnt academically.

Generally:
- [ ] Gathering data, webscraping raw data or via an API
- [ ] Assessing format and quality of data, the conversions required to make analysing possible and straightforward
- [ ] Cleaning utilising programmatic methods to scale for future expansion and editing of unpredictable changes

## Setup
Generally: <br>
- [ ] df_twitter.
- [ ] pip install requests <br>
- [ ] pip install tweepy <br>

`Optional - provides Table Of Contents`
- [ ] pip install jupyter_contrib_nbextensions

## Steps:
1. Read in provided CSV, from subfolder
2. 


## References
- [Name](http link)